---
title: æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•å…¨æ™¯æŒ‡å—
date: 2026-01-23
tags: [äººå·¥æ™ºèƒ½, æ·±åº¦å­¦ä¹ , ä¼˜åŒ–ç®—æ³•, æ•°å­¦å»ºæ¨¡, ç¬¬ä¸€æ€§åŸç†, PyTorch]
categories: [æŠ€æœ¯æ·±åº¦è§£æ, æœºå™¨å­¦ä¹ ]
description: æœ¬æ–‡æ˜¯ä¸€ç¯‡ä¸‡å­—çº§æ·±åº¦ç»¼è¿°ã€‚ä»æ¢¯åº¦çš„ç‰©ç†æœ¬è´¨å‡ºå‘ï¼Œç³»ç»Ÿæ€§å‰–æ SGDã€Momentumã€AdamW åŠ LION ç­‰ç®—æ³•çš„æ¼”è¿›é€»è¾‘ï¼›ç»“åˆâ€œå¹³å¦æå°å€¼â€ç†è®ºæ¢è®¨æ³›åŒ–æ€§å·®å¼‚ï¼Œå¹¶æä¾›å¤§æ¨¡å‹æ—¶ä»£çš„ç”Ÿäº§çº§ä¼˜åŒ–å™¨ä»£ç å®ç°ã€‚
mathjax: true
---


åœ¨äººå·¥æ™ºèƒ½çš„å®ä¼Ÿè“å›¾ä¸­ï¼Œå¦‚æœè¯´ç¥ç»ç½‘ç»œçš„æ¶æ„ï¼ˆArchitectureï¼‰æ˜¯èº¯ä½“ï¼Œæ•°æ®ï¼ˆDataï¼‰æ˜¯è¡€æ¶²ï¼Œé‚£ä¹ˆ **ä¼˜åŒ–ç®—æ³•ï¼ˆOptimization Algorithmï¼‰** å°±æ˜¯é©±åŠ¨è¿™å…·èº¯ä½“ä¸æ–­è¿›åŒ–çš„çµé­‚ã€‚

ä»æ•°å­¦æœ¬è´¨ä¸Šè®²ï¼Œæ·±åº¦å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå°±æ˜¯ä¸€ä¸ªåœ¨é«˜ç»´ã€éå‡¸ï¼ˆNon-convexï¼‰ã€å……æ»¡éç‚¹ï¼ˆSaddle Pointsï¼‰çš„å¤æ‚æµå½¢ä¸Šï¼Œå¯»æ‰¾æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰å…¨å±€æœ€å°å€¼çš„è¿‡ç¨‹ã€‚è¿™ç¯‡æ–‡ç« å°†æ‘’å¼ƒæµ…å±‚çš„ API è°ƒç”¨è®²è§£ï¼Œä»ç¬¬ä¸€æ€§åŸç†å‡ºå‘ï¼Œç³»ç»Ÿæ€§åœ°å‰–æä¼˜åŒ–ç®—æ³•çš„æ¼”è¿›é€»è¾‘ã€æ•°ç†æœ¬è´¨ä»¥åŠåœ¨å½“ä»Šå¤§æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ä»£çš„å®æˆ˜åº”ç”¨ã€‚

![](https://suncos-01-1254144885.cos.ap-shanghai.myqcloud.com/Hexo/clipboard_20260124_012859.png)

---

## ç¬¬ä¸€ç« ï¼šä¼˜åŒ–ç®—æ³•çš„æ¼”è¿›è°±ç³»ä¸å…¨æ™¯åˆ†ç±»

åœ¨æ·±å…¥ç»†èŠ‚ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªæ¸…æ™°çš„åˆ†ç±»åæ ‡ç³»ã€‚ç°ä»£æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•å¹¶éæ‚ä¹±æ— ç« ï¼Œè€Œæ˜¯æ²¿ç€**è§£å†³ç‰¹å®šæ•°å­¦å›°å¢ƒ**çš„è·¯å¾„æ¼”è¿›çš„ã€‚

æˆ‘ä»¬å¯ä»¥å°†ä¸»æµä¼˜åŒ–ç®—æ³•åˆ†ä¸ºå››å¤§æ ¸å¿ƒå®¶æ—ï¼š

### 1.1 æœ´ç´ æ¢¯åº¦ä¸‹é™å®¶æ— (The Gradient Descent Family)
è¿™æ˜¯ä¼˜åŒ–çš„èµ·ç‚¹ã€‚å®ƒä»¬åŸºäºä¸€é˜¶æ³°å‹’å±•å¼€ï¼Œåˆ©ç”¨æ¢¯åº¦ä¿¡æ¯æŒ‡å¯¼å‚æ•°æ›´æ–°ã€‚
* **ä»£è¡¨ç®—æ³•**ï¼šBGD (æ‰¹é‡æ¢¯åº¦ä¸‹é™), SGD (éšæœºæ¢¯åº¦ä¸‹é™), Mini-batch GDã€‚
* **æ ¸å¿ƒé€»è¾‘**ï¼šæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘ï¼Œä¸€æ­¥æ­¥èµ°ä¸‹å±±è°·ã€‚

### 1.2 åŠ¨é‡åŠ é€Ÿå®¶æ— (The Momentum Family)
ä¸ºäº†è§£å†³æœ´ç´ æ¢¯åº¦ä¸‹é™åœ¨â€œå³¡è°·â€åœ°å½¢éœ‡è¡å’Œé™·å…¥å±€éƒ¨æå°å€¼çš„é—®é¢˜ï¼Œå¼•å…¥äº†ç‰©ç†å­¦ä¸­çš„â€œæƒ¯æ€§â€æ¦‚å¿µã€‚
* **ä»£è¡¨ç®—æ³•**ï¼šMomentum (åŠ¨é‡æ³•), Nesterov Accelerated Gradient (NAG, Nesterov åŠ é€Ÿæ¢¯åº¦)ã€‚
* **æ ¸å¿ƒé€»è¾‘**ï¼šä¸ä»…çœ‹å½“å‰çš„å¡åº¦ï¼Œè¿˜å‚è€ƒä¹‹å‰çš„é€Ÿåº¦ï¼Œä»è€Œå†²è¿‡å¹³å¦åŒºï¼ŒæŠ‘åˆ¶éœ‡è¡ã€‚

### 1.3 è‡ªé€‚åº”å­¦ä¹ ç‡å®¶æ— (The Adaptive Learning Rate Family)
ä¸ºäº†è§£å†³ä¸åŒå‚æ•°å¯¹å­¦ä¹ ç‡æ•æ„Ÿåº¦ä¸åŒçš„é—®é¢˜ï¼ˆå³å‚æ•°ç©ºé—´çš„å„å‘å¼‚æ€§ï¼‰ï¼Œå¼•å…¥äº†é’ˆå¯¹æ¯ä¸ªå‚æ•°åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡çš„æœºåˆ¶ã€‚
* **ä»£è¡¨ç®—æ³•**ï¼šAdaGrad, RMSProp, **Adam (è‡ªé€‚åº”çŸ©ä¼°è®¡)**, AdamWã€‚
* **æ ¸å¿ƒé€»è¾‘**ï¼šæ¢¯åº¦çš„æ–¹å·®è¶Šå¤§ï¼ˆæ›´æ–°è¶Šé¢‘ç¹ï¼‰ï¼Œç»™äºˆçš„å­¦ä¹ ç‡è¶Šå°ï¼›åä¹‹åˆ™è¶Šå¤§ã€‚

### 1.4 ç°ä»£é«˜æ•ˆä¸åˆ†å¸ƒå¼å®¶æ— (Modern Efficient & Distributed)
åœ¨å¤§æ¨¡å‹æ—¶ä»£ï¼Œä¸ºäº†åº”å¯¹æ˜¾å­˜ç“¶é¢ˆå’Œè¶…å¤§è§„æ¨¡è®¡ç®—ï¼Œè¡ç”Ÿå‡ºçš„æ–°ä¸€ä»£ç®—æ³•ã€‚
* **ä»£è¡¨ç®—æ³•**ï¼šLION (ç¬¦å·è¿›åŒ–ä¼˜åŒ–), ZeRO (é›¶å†—ä½™ä¼˜åŒ–å™¨)ã€‚
* **æ ¸å¿ƒé€»è¾‘**ï¼šé€šè¿‡é‡åŒ–ã€ç¬¦å·åŒ–æˆ–åˆ†ç‰‡å­˜å‚¨ï¼Œæè‡´å‹ç¼©ä¼˜åŒ–å™¨çš„æ˜¾å­˜å ç”¨ã€‚



---

## ç¬¬äºŒç« ï¼šä»éšæœºæ€§åˆ°ç‰©ç†æƒ¯æ€§ â€”â€” æ¢¯åº¦ä¸‹é™ä¸åŠ¨é‡

### 2.1 éšæœºæ¢¯åº¦ä¸‹é™ (SGD)ï¼šæ··æ²Œä¸­çš„ç§©åº

è™½ç„¶å…¨é‡æ¢¯åº¦ä¸‹é™ï¼ˆBGDï¼‰èƒ½è®¡ç®—å‡ºæœ€å‡†ç¡®çš„ä¸‹é™æ–¹å‘ï¼Œä½†åœ¨æµ·é‡æ•°æ®é¢å‰ï¼Œå…¶è®¡ç®—æˆæœ¬æ˜¯ä¸å¯æ¥å—çš„ã€‚**éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descent, SGDï¼‰** æ¯æ¬¡ä»…éšæœºé‡‡æ ·ä¸€ä¸ªæ ·æœ¬ï¼ˆæˆ–ä¸€ä¸ªå°æ‰¹æ¬¡ï¼‰æ¥ä¼°è®¡æ¢¯åº¦ã€‚

$$ \theta_{t+1} = \theta_t - \eta \cdot \nabla J(\theta_t; x^{(i)}, y^{(i)}) $$

* **ç¬¬ä¸€æ€§åŸç†æ¨å¯¼**ï¼š
    SGD å¼•å…¥çš„â€œå™ªå£°â€åœ¨æŸç§ç¨‹åº¦ä¸Šèµ·åˆ°äº†**éšå¼æ­£åˆ™åŒ–ï¼ˆImplicit Regularizationï¼‰**çš„ä½œç”¨ã€‚è¿™ç§éšæœºæ€§å¯ä»¥å¸®åŠ©æ¨¡å‹è·³å‡ºæµ…å±‚çš„å±€éƒ¨æå°å€¼ï¼ˆLocal Minimaï¼‰ã€‚ç„¶è€Œï¼ŒSGD çš„è·¯å¾„éå¸¸æ›²æŠ˜ï¼Œå°±åƒä¸€ä¸ªé†‰æ±‰åœ¨ä¸‹å±±ã€‚

* **é¢ä¸´çš„æŒ‘æˆ˜ï¼šç—…æ€æ›²ç‡ (Ill-conditioned Curvature)**
    åœ¨æ·±åº¦ç¥ç»ç½‘ç»œçš„æŸå¤±æ›²é¢ä¸Šï¼Œç»å¸¸ä¼šå‡ºç°â€œå³¡è°·â€å½¢çŠ¶â€”â€”åœ¨ä¸€ä¸ªæ–¹å‘ä¸Šå¡åº¦æé™¡ï¼Œè€Œåœ¨å¦ä¸€ä¸ªæ–¹å‘ä¸Šå¡åº¦æç¼“ã€‚SGD ä¼šåœ¨é™¡å³­æ–¹å‘ä¸Šå‰§çƒˆéœ‡è¡ï¼Œè€Œåœ¨å¹³ç¼“æ–¹å‘ä¸Šç§»åŠ¨ç¼“æ…¢ï¼Œå¯¼è‡´æ”¶æ•›é€Ÿåº¦ææ…¢ã€‚



### 2.2 åŠ¨é‡æ³• (Momentum)ï¼šå¼•å…¥ç‰©ç†ä¸–ç•Œçš„æƒ¯æ€§

ä¸ºäº†æŠ‘åˆ¶éœ‡è¡å¹¶åŠ é€Ÿæ”¶æ•›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‰©ç†å­¦ä¸­çš„åŠ¨é‡æ¦‚å¿µã€‚ç°åœ¨çš„æ›´æ–°ä¸ä»…ä»…ä¾èµ–å½“å‰çš„æ¢¯åº¦ï¼Œè¿˜ä¿ç•™äº†ä¸Šä¸€æ­¥çš„æ›´æ–°æ–¹å‘ã€‚

$$ v_t = \gamma v_{t-1} + \eta \nabla J(\theta_t) $$
$$ \theta_{t+1} = \theta_t - v_t $$

* **åŸç†æ·±åº¦è§£æ**ï¼š
    * **ç´¯ç§¯æ•ˆåº”**ï¼šå¦‚æœæ¢¯åº¦æ–¹å‘è¿ç»­å¤šæ¬¡ä¿æŒä¸€è‡´ï¼ˆå¦‚åœ¨ä¸‹å¡è·¯æ®µï¼‰ï¼ŒåŠ¨é‡é¡¹ $v_t$ ä¼šä¸æ–­å¢å¤§ï¼Œä»è€ŒåŠ é€Ÿä¸‹é™ã€‚
    * **å¹³æ»‘æ•ˆåº”**ï¼ˆä½é€šæ»¤æ³¢ï¼‰ï¼šå¦‚æœæ¢¯åº¦æ–¹å‘å‘ç”Ÿæ”¹å˜ï¼ˆå¦‚åœ¨å³¡è°·å£ä¹‹é—´ï¼‰ï¼Œæ­£è´Ÿæ¢¯åº¦ä¼šç›¸äº’æŠµæ¶ˆï¼Œä»è€ŒæŠ‘åˆ¶éœ‡è¡ã€‚
* **é€‚ç”¨åœºæ™¯**ï¼š
    åŠ¨é‡æ³•åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ä»»åŠ¡ï¼ˆå¦‚ ResNet, VGG è®­ç»ƒï¼‰ä¸­ä¾ç„¶è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç»†è°ƒæ•´ SGD ç­–ç•¥ä»¥è·å¾—æ›´å¥½æ³›åŒ–èƒ½åŠ›ï¼ˆGeneralizationï¼‰çš„åœºæ™¯ä¸­ã€‚

### 2.3 Nesterov åŠ é€Ÿæ¢¯åº¦ (NAG)ï¼šå…·å¤‡é¢„åˆ¤èƒ½åŠ›çš„åŠ¨é‡

Nesterov åŠ¨é‡æ˜¯å¯¹æ ‡å‡†åŠ¨é‡æ³•çš„æ”¹è¿›ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šâ€œæ—¢ç„¶æˆ‘ä»¬è¦æŒ‰ç…§åŠ¨é‡æ–¹å‘è¿ˆæ­¥ï¼Œä¸å¦‚å…ˆçœ‹çœ‹è¿ˆå‡ºè¿™ä¸€æ­¥åçš„åœ°å½¢ï¼Œå†å†³å®šæ€ä¹ˆèµ°ã€‚â€

$$ v_t = \gamma v_{t-1} + \eta \nabla J(\theta_t - \gamma v_{t-1}) $$

æ³¨æ„è¿™é‡Œçš„ $\nabla J(\theta_t - \gamma v_{t-1})$ï¼Œå®ƒè®¡ç®—çš„æ˜¯**è¶…å‰ç‚¹**çš„æ¢¯åº¦ã€‚è¿™ç§â€œé¢„åˆ¤â€èƒ½åŠ›ä½¿å¾— NAG åœ¨æ¥è¿‘æå€¼ç‚¹æ—¶èƒ½å¤Ÿæå‰å‡é€Ÿï¼Œå…·æœ‰æ›´å¼ºçš„æ”¶æ•›ç¨³å®šæ€§ã€‚

---

## ç¬¬ä¸‰ç« ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ â€”â€” ä¸ºæ¯ä¸ªå‚æ•°å®šåˆ¶â€œæ­¥ä¼â€

åœ¨å¤„ç†ç¨€ç–æ•°æ®ï¼ˆå¦‚ NLP ä¸­çš„è¯å‘é‡ï¼‰æˆ–æ·±å±‚ç½‘ç»œæ—¶ï¼Œä¸åŒå‚æ•°çš„æ¢¯åº¦é‡çº§å¯èƒ½ç›¸å·®æ•°ä¸ªæ•°é‡çº§ã€‚å¯¹æ‰€æœ‰å‚æ•°ä½¿ç”¨åŒä¸€ä¸ªå­¦ä¹ ç‡ï¼ˆGlobal Learning Rateï¼‰æ˜¾ç„¶ä¸å†æ˜¯æœ€ä¼˜è§£ã€‚

### 3.1 AdaGradï¼šå†å²çš„ç§¯ç´¯ä¸æƒ©ç½š

AdaGrad (Adaptive Gradient Algorithm) æ˜¯è‡ªé€‚åº”ç®—æ³•çš„å…ˆé©±ã€‚å®ƒé€šè¿‡ç´¯ç§¯å†å²æ¢¯åº¦çš„å¹³æ–¹å’Œï¼Œæ¥åŠ¨æ€è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ã€‚

$$ G_{t, ii} = \sum_{\tau=1}^t (g_{\tau, i})^2 $$
$$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i} $$

* **åŸç†**ï¼šé¢‘ç¹æ›´æ–°çš„å‚æ•°ï¼ˆæ¢¯åº¦å¤§ï¼‰ï¼Œå…¶ $G_t$ å¤§ï¼Œå­¦ä¹ ç‡è¢«æ˜¾è‘—é™ä½ï¼›ç¨€ç–æ›´æ–°çš„å‚æ•°ï¼Œå­¦ä¹ ç‡ä¿æŒè¾ƒå¤§ã€‚
* **ç¼ºé™·**ï¼š$G_t$ æ˜¯å•è°ƒé€’å¢çš„ï¼Œå¯¼è‡´è®­ç»ƒåæœŸå­¦ä¹ ç‡ä¼šè¶‹è¿‘äº 0ï¼Œæ¨¡å‹æå‰åœæ­¢å­¦ä¹ ã€‚

### 3.2 RMSPropï¼šé—å¿˜çš„è‰ºæœ¯

ä¸ºäº†è§£å†³ AdaGrad å­¦ä¹ ç‡è¿‡æ—©æ¶ˆå¤±çš„é—®é¢˜ï¼ŒGeoff Hinton æå‡ºäº† RMSPropã€‚å®ƒå¼•å…¥äº†**æŒ‡æ•°ç§»åŠ¨å¹³å‡ (Exponential Moving Average, EMA)** æ¥è®¡ç®—äºŒé˜¶çŸ©ã€‚

$$ E[g^2]_t = \beta E[g^2]_{t-1} + (1 - \beta) g_t^2 $$

è¿™ä½¿å¾—ç®—æ³•åªå…³æ³¨â€œæœ€è¿‘â€çš„æ¢¯åº¦æƒ…å†µï¼Œä»è€Œæ‘†è„±äº†å†å²æ¢¯åº¦çš„æ²‰é‡åŒ…è¢±ã€‚

### 3.3 Adamï¼šé›†å¤§æˆè€… (Adaptive Moment Estimation)

**Adam** æ— ç–‘æ˜¯ç›®å‰æœ€æµè¡Œçš„ä¼˜åŒ–ç®—æ³•ã€‚å®ƒç»“åˆäº† Momentum çš„ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆæ¢¯åº¦çš„å‡å€¼ï¼‰å’Œ RMSProp çš„äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆæ¢¯åº¦çš„æœªä¸­å¿ƒåŒ–æ–¹å·®ï¼‰ã€‚

* **æ•°å­¦å½¢å¼**ï¼š
    1.  **ä¸€é˜¶çŸ© (Momentum)**: $m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$
    2.  **äºŒé˜¶çŸ© (Adaptive)**: $v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$
    3.  **åå·®ä¿®æ­£ (Bias Correction)**: ç”±äº $m_0, v_0$ åˆå§‹åŒ–ä¸º 0ï¼ŒåˆæœŸä¼°å€¼åå‘ 0ï¼Œå› æ­¤éœ€è¦é™¤ä»¥ $(1-\beta^t)$ è¿›è¡Œä¿®æ­£ã€‚

* **AdamW çš„ä¿®æ­£ï¼šL2 æ­£åˆ™ä¸æƒé‡è¡°å‡çš„è§£è€¦**
    åœ¨ Adam ä¸­ä½¿ç”¨ L2 æ­£åˆ™åŒ–ä¸æƒé‡è¡°å‡ï¼ˆWeight Decayï¼‰å¹¶ä¸ç­‰ä»·ã€‚**AdamW** å°†æƒé‡è¡°å‡é¡¹ä»æ¢¯åº¦æ›´æ–°ä¸­è§£è€¦å‡ºæ¥ï¼Œç›´æ¥ä½œç”¨äºå‚æ•°æœ¬èº«ï¼š
    $$ \theta_{t+1} = \theta_t - \eta (\dots) - \eta \lambda \theta_t $$
    è¿™ä¸€æ”¹åŠ¨åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ BERT, GPTï¼‰å’Œ Transformer æ¶æ„çš„è®­ç»ƒä¸­è‡³å…³é‡è¦ï¼Œæ˜¯ç›®å‰ LLM è®­ç»ƒçš„æ ‡å‡†é…ç½®ã€‚

### 3.4 LION (EvoLved Sign Momentum)ï¼šç¬¦å·åŒ–çš„è¿›åŒ–å¥‡ç‚¹

2023 å¹´ï¼ŒGoogle Brain é€šè¿‡ç¨‹åºè‡ªåŠ¨æœç´¢ï¼ˆAutoMLï¼‰å‘ç°äº†ä¸€ä¸ªæå…¶ç®€å•çš„ç®—æ³•â€”â€”**LION**ã€‚ä¸ Adam å­˜å‚¨å¤æ‚çš„æ¢¯åº¦å€¼ä¸åŒï¼ŒLION **åªå…³æ³¨æ¢¯åº¦çš„ç¬¦å·ï¼ˆæ­£æˆ–è´Ÿï¼‰**ã€‚

$$ \theta_{t+1} = \theta_{t} - \eta \cdot \text{sign}(\beta_1 m_{t-1} + (1-\beta_1)g_t) $$

* **ç¬¬ä¸€æ€§åŸç†**ï¼šå®ƒæŠ›å¼ƒäº†æ¢¯åº¦çš„â€œå¤§å°â€ä¿¡æ¯ï¼Œåªä¿ç•™â€œæ–¹å‘â€ã€‚è¿™åœ¨æ•°ç†ä¸Šç±»ä¼¼äºå°†ä¼˜åŒ–è¿‡ç¨‹çº¦æŸåœ¨ä¸€ä¸ªè¶…ç«‹æ–¹ä½“çš„é¡¶ç‚¹ä¸Šç§»åŠ¨ã€‚
* **æ ¸å¿ƒä¼˜åŠ¿**ï¼š
    1.  **çœæ˜¾å­˜**ï¼šä¸éœ€è¦å­˜å‚¨äºŒé˜¶çŸ© $v_t$ï¼Œæ˜¾å­˜å ç”¨æ¯” AdamW å°‘ 50%ã€‚
    2.  **æ³›åŒ–å¼º**ï¼šåœ¨ Vision Transformer å’Œ Diffusion Model ä¸Šè¡¨ç°ä¼˜äº AdamWã€‚

---

## ç¬¬å››ç« ï¼šç®—æ³•æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”ä¸åœºæ™¯é€‰æ‹©

â€œæ²¡æœ‰å…è´¹çš„åˆé¤å®šç†â€ï¼ˆNo Free Lunch Theoremï¼‰å‘Šè¯‰æˆ‘ä»¬ï¼Œä¸å­˜åœ¨ä¸€ç§åœ¨æ‰€æœ‰é—®é¢˜ä¸Šéƒ½è¡¨ç°æœ€ä¼˜çš„ç®—æ³•ã€‚ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©ï¼Œå¾€å¾€æ˜¯åœ¨**æ”¶æ•›é€Ÿåº¦**ã€**æ³›åŒ–èƒ½åŠ›**å’Œ**è®¡ç®—èµ„æº**æ„æˆçš„â€œä¸å¯èƒ½ä¸‰è§’â€ä¸­å¯»æ‰¾å¹³è¡¡ã€‚

### 4.1 æ ¸å¿ƒç®—æ³•å¤šç»´åº¦å¯¹æ¯”è¡¨

| ç®—æ³•åç§° | æ”¶æ•›é€Ÿåº¦ | æ³›åŒ–èƒ½åŠ› | æ˜¾å­˜å¼€é”€ | å¯¹è¶…å‚æ•æ„Ÿåº¦ | æ ¸å¿ƒç‰¹æ€§ä¸€å¥è¯æ€»ç»“ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **SGD** | æ…¢ (Slow) | â­â­â­â­â­ (æå¼º) | â­ (æä½) | é«˜ (éš¾è°ƒ) | ä¾é éšæœºå™ªå£°å¯»æ‰¾å¹³å¦æå°å€¼ï¼ŒCV é¢†åŸŸçš„â€œè€å°†â€ã€‚ |
| **Momentum** | ä¸­ (Medium) | â­â­â­â­ (å¼º) | â­ (ä½) | ä¸­ | å¼•å…¥ç‰©ç†æƒ¯æ€§ç©¿è¶Šéç‚¹ï¼ŒSGD çš„å¼ºåŠ›å‡çº§ç‰ˆã€‚ |
| **AdaGrad** | å¿« (å‰æœŸ) | â­â­ (è¾ƒå¼±) | â­â­ (ä¸­) | ä½ | é€‚åˆç¨€ç–æ•°æ®ï¼Œä½†å­¦ä¹ ç‡è¿‡æ—©è¡°å‡å¯¼è‡´åæœŸæ— åŠ›ã€‚ |
| **AdamW** | æå¿« (Fast) | â­â­â­ (ä¸­/å¼º*) | â­â­â­ (é«˜) | æä½ (å¥½è°ƒ) | **å·¥ä¸šç•Œé»˜è®¤é¦–é€‰**ã€‚AdamW ä¿®å¤äº†æƒé‡è¡°å‡é—®é¢˜ï¼Œæˆä¸º LLM æ ‡é…ã€‚ |
| **LION** | æå¿« | â­â­â­â­ (å¼º) | â­â­ (è¾ƒä½) | ä½ | Google æ–°å® ï¼Œä»…ç”¨ç¬¦å·ä¿¡æ¯ï¼Œæ¯” AdamW çœæ˜¾å­˜ä¸”æ³›åŒ–æ›´å¥½ã€‚ |

> **æ³¨**ï¼š*Adam åœ¨é…åˆæ­£ç¡®çš„æƒé‡è¡°å‡ï¼ˆå³ AdamWï¼‰å’Œå­¦ä¹ ç‡é¢„çƒ­ï¼ˆWarmupï¼‰åï¼Œæ³›åŒ–èƒ½åŠ›å¯é€¼è¿‘ SGDã€‚

### 4.2 æ·±åº¦è¾©è®ºï¼šæ³›åŒ–å·®è· (Generalization Gap) ä¹‹è°œ

#### ä¸ºä»€ä¹ˆ CVï¼ˆè®¡ç®—æœºè§†è§‰ï¼‰åçˆ± SGDï¼Ÿ
ç ”ç©¶è¡¨æ˜ï¼Œè‡ªé€‚åº”ç®—æ³•ï¼ˆAdam ç­‰ï¼‰å€¾å‘äºæ”¶æ•›åˆ°**å°–é”æå°å€¼ï¼ˆSharp Minimaï¼‰**ï¼Œè€Œ SGD å€¾å‘äºæ”¶æ•›åˆ°**å¹³å¦æå°å€¼ï¼ˆFlat Minimaï¼‰**ã€‚
* **å°–é”æå°å€¼**ï¼šè®­ç»ƒé›† Loss å¾ˆä½ï¼Œä½†æµ‹è¯•æ•°æ®ç¨æœ‰åˆ†å¸ƒåç§»ï¼ŒLoss å°±ä¼šå‰§å¢ï¼ˆæ³›åŒ–å·®ï¼‰ã€‚
* **å¹³å¦æå°å€¼**ï¼šå‘¨å›´åŒºåŸŸ Loss éƒ½å¾ˆä½ï¼Œå¯¹æ•°æ®åç§»å…·æœ‰é²æ£’æ€§ï¼ˆæ³›åŒ–å¥½ï¼‰ã€‚
å› æ­¤ï¼Œåœ¨ ResNet, YOLO ç­‰è§†è§‰æ¨¡å‹è®­ç»ƒä¸­ï¼Œ**SGD + Momentum + Cosine Decay** ä¾ç„¶æ˜¯åˆ·æ¦œæ ‡é…ã€‚


#### ä¸ºä»€ä¹ˆ NLP ä¸ LLM å¿…é¡»ç”¨ AdamWï¼Ÿ
Transformer æ¶æ„å±‚çº§æ·±ã€å‚æ•°åˆ†å¸ƒåŠ¨æ€èŒƒå›´æå¤§ã€‚SGD åœ¨è¿™ç§å¤æ‚çš„æŸå¤±åœ°å½¢ä¸Šæéš¾è°ƒè¯•ï¼Œå®¹æ˜“é­é‡æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚AdamW çš„è‡ªé€‚åº”å­¦ä¹ ç‡èƒ½å¤Ÿè‡ªåŠ¨å¹³æ»‘ä¸åŒå±‚çº§çš„æ¢¯åº¦å·®å¼‚ï¼Œæå¤§åœ°ç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹ã€‚

### 4.3 åœºæ™¯åŒ–é€‰å‹æŒ‡å—

1.  **CNN / ResNet / å›¾åƒåˆ†ç±»**ï¼šé¦–é€‰ `SGD + Momentum`ã€‚è¿½æ±‚æè‡´ç²¾åº¦ã€‚
2.  **Transformer / BERT / GPT / Llama**ï¼šé¦–é€‰ `AdamW`ã€‚å¿…é¡»ä¿è¯æ”¶æ•›ç¨³å®šæ€§ã€‚
3.  **å¾®è°ƒ (Fine-tuning)**ï¼š`AdamW` æˆ– `RMSProp`ã€‚å¿«é€Ÿé€‚é…æ–°æ•°æ®ã€‚
4.  **æ˜¾å­˜æåº¦å—é™ (LoRA/QLoRA)**ï¼šé¦–é€‰ `PagedAdamW (8-bit)` æˆ– `LION`ã€‚

---

## ç¬¬äº”ç« ï¼šä»£ç å®æˆ˜ â€”â€” å¯è§†åŒ–ä¼˜åŒ–è½¨è¿¹ä¸æ”¶æ•›å¯¹æ¯”

ç†è®ºçš„ç»ˆç‚¹æ˜¯ä»£ç ï¼Œè€Œä»£ç çš„ç»ˆç‚¹æ˜¯â€œå¯è§æ€§â€ã€‚ä¸ºäº†çœŸæ­£ç†è§£ SGD ä¸ AdamW çš„åŒºåˆ«ï¼Œæˆ‘ä»¬ä¸ä»…è¦çœ‹æœ€ç»ˆçš„ Lossï¼Œæ›´è¦çœ‹å®ƒä»¬æ˜¯å¦‚ä½•â€œä¸‹å±±â€çš„ã€‚

ä»¥ä¸‹ä»£ç åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š
1.  **Trainer ç±»**ï¼šç”Ÿäº§çº§çš„è®­ç»ƒé€»è¾‘ï¼ŒåŒ…å«æ¢¯åº¦è£å‰ªä¸ Warmupã€‚
2.  **Visualizer è„šæœ¬**ï¼šåœ¨ä¸€ä¸ªæ¨¡æ‹Ÿçš„ 2D æŸå¤±æ›²é¢ä¸Šï¼Œç›´è§‚å¯¹æ¯”ä¸åŒç®—æ³•çš„å¯»ä¼˜è·¯å¾„ã€‚

### 5.1 æ„å»ºç”Ÿäº§çº§ä¼˜åŒ–é—­ç¯

```python
import torch
import torch.nn as nn
from torch.optim import AdamW, SGD
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import matplotlib.pyplot as plt
import numpy as np

class Trainer:
    def __init__(self, model, optimizer_type='adamw', lr=1e-3):
        self.model = model
        self.losses = []  # ç”¨äºè®°å½• Loss è½¨è¿¹
        
        # 1. å‚æ•°åˆ†ç»„ï¼šæƒé‡è¡°å‡è§£è€¦ (Weight Decay Decoupling)
        # ç¬¬ä¸€æ€§åŸç†ï¼šLayerNorm å’Œ Bias ä¸åº”è¢«æ­£åˆ™åŒ–è¡°å‡ï¼Œå¦åˆ™ä¼šç ´åæ•°å€¼ç¨³å®šæ€§
        param_optimizer = list(model.named_parameters())
        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 
             'weight_decay': 0.01},
            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 
             'weight_decay': 0.0}
        ]

        # 2. ä¼˜åŒ–å™¨å·¥å‚
        if optimizer_type.lower() == 'adamw':
            # Betas=(0.9, 0.95) æ˜¯ LLaMA ç­‰å¤§æ¨¡å‹è®­ç»ƒçš„ç»éªŒå‚æ•°
            self.optimizer = AdamW(optimizer_grouped_parameters, lr=lr, betas=(0.9, 0.95))
        elif optimizer_type.lower() == 'sgd':
            self.optimizer = SGD(optimizer_grouped_parameters, lr=lr, momentum=0.9)
            
        # 3. å­¦ä¹ ç‡è°ƒåº¦ï¼šä½™å¼¦é€€ç«
        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=50, T_mult=2)

    def train_step(self, data, target):
        self.model.train()
        self.optimizer.zero_grad()
        
        output = self.model(data)
        loss = nn.MSELoss()(output, target)
        loss.backward()

        # 4. æ¢¯åº¦è£å‰ª (Gradient Clipping)
        # é˜²æ­¢é«˜ç»´å³¡è°·åœ°å½¢ä¸­çš„æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

        self.optimizer.step()
        self.scheduler.step()
        
        self.losses.append(loss.item())
        return loss.item()
```

### 5.2 è§è¯å·®å¼‚ï¼šSGD vs AdamW çš„è½¨è¿¹å¯è§†åŒ–

ä¸ºäº†ç›´è§‚å±•ç¤ºâ€œéšæœºæ€§â€ä¸â€œè‡ªé€‚åº”â€çš„åŒºåˆ«ï¼Œæˆ‘ä»¬æ„å»ºä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿå®éªŒï¼šè®©ä¸¤ä¸ªä¼˜åŒ–å™¨å»æ‹Ÿåˆç®€å•çš„çº¿æ€§æ•°æ®ï¼Œå¹¶ç”»å‡º Loss ä¸‹é™æ›²çº¿ã€‚

```python
def visualize_comparison():
    # æ¨¡æ‹Ÿæ•°æ®ï¼šy = 2x + 1
    X = torch.linspace(-1, 1, 100).view(-1, 1)
    Y = 2 * X + 1 + torch.randn(X.size()) * 0.1 # åŠ å…¥å™ªå£°
    
    # å®šä¹‰ç®€å•æ¨¡å‹
    model_sgd = nn.Linear(1, 1)
    model_adam = nn.Linear(1, 1)
    
    # ä¿æŒåˆå§‹æƒé‡ä¸€è‡´ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”
    with torch.no_grad():
        model_adam.weight.copy_(model_sgd.weight)
        model_adam.bias.copy_(model_sgd.bias)

    # å®ä¾‹åŒ–è®­ç»ƒå™¨
    trainer_sgd = Trainer(model_sgd, optimizer_type='sgd', lr=0.1)
    trainer_adam = Trainer(model_adam, optimizer_type='adamw', lr=0.1)

    print("å¼€å§‹å¯¹æ¯”è®­ç»ƒ...")
    steps = 100
    for i in range(steps):
        trainer_sgd.train_step(X, Y)
        trainer_adam.train_step(X, Y)

    # --- ç»˜å›¾é€»è¾‘ ---
    plt.figure(figsize=(12, 5))
    
    # ç»˜åˆ¶ Loss æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(trainer_sgd.losses, label='SGD + Momentum', color='blue', alpha=0.7)
    plt.plot(trainer_adam.losses, label='AdamW', color='red', linestyle='--', linewidth=2)
    plt.title('Convergence Speed Comparison')
    plt.xlabel('Steps')
    plt.ylabel('Loss (MSE)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # ç»˜åˆ¶æ‹Ÿåˆç»“æœ
    plt.subplot(1, 2, 2)
    plt.scatter(X.numpy(), Y.numpy(), color='gray', alpha=0.5, label='Data')
    plt.plot(X.numpy(), model_sgd(X).detach().numpy(), 'b-', label='SGD Fit')
    plt.plot(X.numpy(), model_adam(X).detach().numpy(), 'r--', label='AdamW Fit')
    plt.title('Final Model Fit')
    plt.legend()
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    visualize_comparison()
```

#### ğŸ’¡ ç»“æœè§£è¯» (Result Interpretation)

è¿è¡Œä¸Šè¿°ä»£ç ï¼Œé€šè¿‡è§‚å¯Ÿ Loss æ›²çº¿ï¼Œæˆ‘ä»¬å¯ä»¥éªŒè¯ä»¥ä¸‹æ·±å±‚è§„å¾‹ï¼ˆåŸºäºå…¸å‹å®éªŒç»“æœï¼‰ï¼š
![](https://suncos-01-1254144885.cos.ap-shanghai.myqcloud.com/Hexo/clipboard_20260124_011636.png)

1.  **SGD çš„â€œè½æ’â€ä¸éœ‡è¡**ï¼š
    * åœ¨ç®€å•å‡¸ä¼˜åŒ–ä»»åŠ¡ä¸­ï¼Œé…ç½®äº†åŠ¨é‡ï¼ˆMomentumï¼‰çš„ SGD ä¸‹é™æå¿«ï¼ˆè“çº¿ï¼‰ï¼Œåœ¨åˆæœŸç”šè‡³å¯èƒ½è¶…è¿‡ AdamWã€‚
    * **å…³é”®ç»†èŠ‚**ï¼šæ³¨æ„è“çº¿åœ¨åº•éƒ¨çš„**æ³¢åŠ¨ï¼ˆOscillationï¼‰**ã€‚ç”±äº SGD å¯¹æ¢¯åº¦çš„ç¼©æ”¾æ˜¯çº¿æ€§çš„ï¼ŒåŠ¨é‡è¿‡å¤§å®¹æ˜“å¯¼è‡´å®ƒå‡ºç°â€œè¿‡å†²â€ç°è±¡â€”â€”å†²è¿‡è°·åº•å†æŠ˜è¿”ã€‚åœ¨å¤æ‚çš„æ·±å±‚ç½‘ç»œä¸­ï¼Œè¿™ç§éœ‡è¡å¯èƒ½å¯¼è‡´æ¨¡å‹æ— æ³•æ”¶æ•›ã€‚

2.  **AdamW çš„â€œç¨³å¥â€ä¸å¹³æ»‘**ï¼š
    * çº¢çº¿ï¼ˆAdamWï¼‰å±•ç¤ºäº†æä½³çš„ç¨³å®šæ€§ã€‚ä¾é è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå®ƒæ ¹æ®æ¢¯åº¦çš„æ–¹å·®åŠ¨æ€è°ƒæ•´æ­¥é•¿ã€‚
    * å³ä¾¿åœ¨åˆæœŸä¸‹é™é€Ÿåº¦ä¸ SGD æŒå¹³æˆ–ç¨æ…¢ï¼ŒAdamW ä¹Ÿèƒ½**å¹³æ»‘åœ°ï¼ˆSmoothlyï¼‰** ç€é™†ï¼Œæå°‘å‡ºç°å‰§çƒˆéœ‡è¡ã€‚è¿™ç§ç‰¹æ€§åœ¨ Transformer ç­‰å¯¹è¶…å‚æ•æ„Ÿçš„å¤§æ¨¡å‹è®­ç»ƒä¸­æ˜¯å†³å®šæ€§çš„ä¼˜åŠ¿ã€‚

---

## ç»“è¯­ï¼šåœ¨æ··æ²Œä¸­å¯»æ‰¾æœ€ä¼˜è§£

å›é¡¾æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•çš„æ¼”è¿›å²ï¼Œæˆ‘ä»¬çœ‹åˆ°çš„ä¸ä»…æ˜¯æ•°å­¦å…¬å¼çš„å †ç Œï¼Œè€Œæ˜¯äººç±»è¯•å›¾é©¯æœé«˜ç»´æ··æ²Œçš„æ™ºæ…§ã€‚

* **SGD** å‘Šè¯‰æˆ‘ä»¬ï¼Œ**éšæœºæ€§**ï¼ˆRandomnessï¼‰å¹¶éå™ªéŸ³ï¼Œè€Œæ˜¯è·³å‡ºå±€éƒ¨æœ€ä¼˜çš„é˜¶æ¢¯ã€‚
* **Momentum** å‘Šè¯‰æˆ‘ä»¬ï¼Œ**å†å²**ï¼ˆHistoryï¼‰æ˜¯æœ‰ä»·å€¼çš„ï¼Œä¿æŒæƒ¯æ€§æ‰èƒ½è·¨è¶Šä½è°·ã€‚
* **Adam** å‘Šè¯‰æˆ‘ä»¬ï¼Œ**é€‚åº”æ€§**ï¼ˆAdaptabilityï¼‰æ˜¯å…³é”®ï¼Œå¯¹ä¸åŒçš„ç»´åº¦è¦ç»™äºˆä¸åŒçš„å…³æ³¨ã€‚
* **LION** å‘Šè¯‰æˆ‘ä»¬ï¼Œ**ç®€æ´**ï¼ˆSimplicityï¼‰å¾€å¾€è•´å«ç€æœ€å¼ºçš„æ³›åŒ–åŠ›é‡ã€‚

åœ¨å¤§æ¨¡å‹è¿ˆå‘ AGI çš„å¾é€”ä¸­ï¼Œä¼˜åŒ–ç®—æ³•ä¾ç„¶åœ¨è¿›åŒ–ã€‚ä» LION çš„ç¬¦å·åŒ–å°è¯•ï¼Œåˆ° ZeRO çš„åˆ†å¸ƒå¼æ‹†è§£ï¼Œæœªæ¥çš„ä¼˜åŒ–å™¨å°†æ›´åŠ å…³æ³¨**è®¡ç®—æ•ˆç‡**ä¸**ç¡¬ä»¶ååŒ**ã€‚æŒæ¡è¿™äº›åº•å±‚é€»è¾‘ï¼Œä½ å°±æŒæ¡äº†è°ƒæ•™ AI çš„æ ¸å¿ƒé’¥åŒ™ã€‚

---